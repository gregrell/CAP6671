# -*- coding: utf-8 -*-
"""GregRell_CombinedArms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13wHG2jSSyvshv6nHShO_pKMum2NboxEz

# Introduction
This code example uses [PettingZoo](https://www.pettingzoo.ml/#) as a mutli-agent reinforcement learning platform for the [Combinded Arms](https://www.pettingzoo.ml/magent/combined_arms) environment.

This is an example of how to render PettingZoo's GUI in Colab and also serves as a template for testing separate Red and Blue models.

Author: Dr. S. Mondesire

Last Modified: 2022-04-03

# Import Libraries
"""

!apt update && apt install xvfb
!pip install gym-notebook-wrapper
!apt update && apt install python-opengl ffmpeg
!pip install pygame
!pip install pettingzoo[magent]

"""# Model Creation
This model randomly moves, attacks, or does nothing per timestep. Handles both ranged and melee agents.
"""

#from _typeshed import Self
from os import name
class MyModel:
  
  # Imports for ML libraries
  import math

  def __init__(self,name):
    self.name=name
 
  def predict(self, env, observation, agent):
    action = env.action_space(agent).sample()
    #print(f"name {self.name} {self.math.cos(59)}")

    return action
  
  def train(self, env, observations, rewards, dones, infos, actions):


    
    return False







# This is the defaut random action class. Used as benchmark
class RandModel:    
  def __init__(self,name):
    self.name=name
 
  def predict(self, env, observation, agent):
    action = env.action_space(agent).sample()

    return action

"""# Test Model"""

import gym
import gnwrapper
from pettingzoo.magent import combined_arms_v5
import pickle



MAX_TIMESTEPS = 500

# Initialize the Combined Arms environment
env = combined_arms_v5.parallel_env() # Parallel environemnt
env = gnwrapper.LoopAnimation(env) # Start Xvfb

# Instantiate models
# redModel = MyModel()
redModel = MyModel("Red")
blueModel = RandModel("Blue")

# Save (pickle) model
pickle.dump(redModel, open("redModel.pkl", 'wb') )

# Load pickled model
with open("redModel.pkl", 'rb') as pickle_model:
  redModel = pickle.load(pickle_model)
  #blueModel = pickle.load(pickle_model)

observations = env.reset()

for step in range(MAX_TIMESTEPS):
  actions = {}

  # For each agent, get the next action
  for agent in env.agents:
    model = None
    if "red" in agent:
      model = redModel
    elif "blue" in agent:
      model = blueModel
    
    actions.update({agent: model.predict(env, observations[agent], agent) })

  observations, rewards, dones, infos = env.step(actions)

  # display some stats
  #print(f"Observation {observations.get(agent)} reward {rewards.get(agent)} done {dones.get(1)} info {infos.get(1)}")
  
  env.render() # Render the current frame

# Display the rendered movie
env.display()

# Final Stats
redMelee = 0
redRanged = 0
blueMelee = 0
blueRanged = 0
for agent in env.agents:
  #print(f"{agent.name}")
  if "redmelee" in agent:
    redMelee += 1
  elif "redranged" in agent:
    redRanged += 1
  elif "bluemele" in agent:
    blueMelee += 1
  elif "blueranged" in agent:
    blueRanged += 1

print("Red Total {}: (Melee {}, Ranged {})".format(redMelee+redRanged,redMelee, redRanged) )
print("Blue Total {}: (Melee {}, Ranged {})".format(blueMelee+blueRanged,blueMelee, blueRanged) )